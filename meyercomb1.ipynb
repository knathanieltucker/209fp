{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bayesian Tweets:  Twitter Sentiment Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we load in some standard packages and a couple of packages we wrote ourselves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#ours\n",
      "import twitter3 as tw3\n",
      "import performance1 as perf1\n",
      "import performance2 as perf2\n",
      "import sentiment1 as sent\n",
      "#standard\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from pattern.web import Element\n",
      "import requests\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from scipy import sparse\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "#import ystockquote as ysq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next two cells scrape wikipedia for the ticker symbols of the S&P 500 companies, our stock data set we'll be using, and also prepare the list of dates we'll try to use for our analysis.  Notably, the Twitter API only allows us to go back about a week.  We've commented out the first cell and just loaded the symbols from a csv file in the second cell"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "# spy = requests.get('http://en.wikipedia.org/wiki/List_of_S&P_500_companies').text\n",
      "# tickers = []\n",
      "# dom = Element(spy)\n",
      "# for a in dom('tr td:first-child a'):\n",
      "#     ticker = a.content\n",
      "#     if len(ticker) < 5:\n",
      "#         tickers.append(str(ticker))\n",
      "# tickers.append('BRK.B')\n",
      "# tickers.append('CMCSA')\n",
      "# tickers.append('DISCA')\n",
      "# tickers = np.sort(tickers)\n",
      "#tickerlist = tickers\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spx = pd.read_csv('SPXSymbolsPD.csv', delimiter=',')\n",
      "tickerlist = list(spx['Ticker'])\n",
      "datelist = [datetime.date(2013,12,5),datetime.date(2013,12,6),datetime.date(2013,12,7),datetime.date(2013,12,8),datetime.date(2013,12,9),datetime.date(2013,12,10)]\n",
      "weekdays_datetime = [datetime.date(2013,12,5),datetime.date(2013,12,6),datetime.date(2013,12,9),datetime.date(2013,12,10)]\n",
      "weekdays_str = [str(day) for day in weekdays_datetime]\n",
      "weekend_datetime = [datetime.date(2013,12,7),datetime.date(2013,12,8)]\n",
      "weekend_str = [str(day) for day in weekend_datetime]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to use the Twitter API to search for the tweets including each stock ticker symbol on each day.  We take all the tweets we can find, or 100 tweets, whichever is smaller.  We wrote all of the functions in tw3, although we do use other packages (twython and twitter) designed to interface with the Twitter API.  We initially made extensive use of twitter and then switched to twython as it gave us better error handling.  As we want to do build this entire data set in one call, we have to be able to handle a wide variety of errors, such as dealing with our rate limits and many of the small issues that could crop up during a single API call.  We also make use of extensive printing so we can monitor the function as it runs.  We've commented it out and loaded the data from csv so as to avoid the 1.5 hour process of performing the searches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loadfull = tw3.safemultisearch(tickerlist,datelist)\n",
      "#loadfull.to_csv('data.csv')\n",
      "data = pd.read_csv('data.csv')\n",
      "#remove weekend days as the market is closed\n",
      "data = data[data['date']!=weekend_str[0]]\n",
      "data = data[data['date']!=weekend_str[1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we need to check the stock performance on each day, so we wrote another function to add a 0 wherever the stock went down and a 1 wherever it went up.  We use the ystockquote package to assist in gathering the stock data.  Again, we comment this out to limit ourselves to a single round of data collection and load the results from csv."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data,performance = perf1.check_performance(data)\n",
      "#data = data[data['performance']>-1]#only keep if performance is 0 or 1, not NaN\n",
      "#data.to_csv('data_perf.csv')\n",
      "data = pd.read_csv('data_perf.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have a data set!  We can go ahead and print part of our dataframe to see what sort of information we've stored, namely the symbol of the company, the date in question, text from the twitter searches, and a performance indicator.  We made the choice to treat the entire twitter corpus for each stock from each day as a single bag of words, so the text here is the concatenated tweets (converted to ascii and without punctuation) of the entire result of each particular search.  We'll turn it into a proper bag of words a bit later on.\n",
      "\n",
      "This differs from rotten tomatos where we kept each review separate, but in that case we also had a \"fresh\" or \"rotten\" indicator for each review.  Here we only have the stock performance, which is obviously doesn't change depending on which tweet we're looking at, so we just combine all of the tweets into a single bag of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data.irow(0)\n",
      "print data.irow(0)['text']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unnamed: 0                                                      0\n",
        "Unnamed: 0.1                                                  MMM\n",
        "company                                                       MMM\n",
        "date                                                   2013-12-05\n",
        "text            MMM Could 3M Dare Trump GE on Dividend Hikes h...\n",
        "performance                                                     0\n",
        "Name: 0, dtype: object\n",
        "MMM Could 3M Dare Trump GE on Dividend Hikes httptcoG7ukoGT5kECould 3M Dare Trump GE on Dividend Hikes MMM httptco4QBxAilSRDRT MScharts TSN MAT MMM LH Portfolios Pops amp Drops  httptco9hMj9HZuCGBarclays has estimated MMM target price at 121 defensive on 3Ms ability to maintain margins  httptcofvmByxwClFLaMonicaBuzz MMM  Buy on the dip httptcoxVMwKU07SsGeoffrey2313819 Ha Did 3M sponsor that tweet MMMSampP100 Stocks Performance INTC BA SPG NSC EBAY DVN UNP EMC MA GILD F MMM LMT ABBV AAPL  more httptcon4QZIDxy7wTSN MAT MMM LH Portfolios Pops amp Drops  httptco9hMj9HZuCGOne more small order filled Sold 1 Dec13 MMM 120 put for 37 centsTSN MAT MMM LH Portfolios Pops amp Drops  httptcoeB1gLFQgbDTSN MAT MMM LH Portfolios Pops amp Drops  httptcokqZTxc7ofATSN MAT MMM LH Portfolios Pops amp Drops  httptco2z5afpz5SrAnalyst estimate MMM EPS at 672 which is 2 above second quarter estimates  httptcofvmByxwClF httptco5wqB2bnXnFMMM mean target price is 127 as Barclays Credit Suisse and Citi gave stock neutral ratings  httptcoDtPqdKgHjrOptionSniper1 On my list to look at GOGO SLW P AAPL IBM OXY MMM IYR TWTR Didnt get to any yday Wanted to sell MSFT callsMMM breaking out3M Co The stock is testing its highs MMM httptcodS64sIMoSi httptcoZE6oMDONO0Pennystock Research on DCIN MMM GFIG XPO SHOR AIN View now httptcotCMpiAEdzGLooking for winners like ENV STXS RNIN BPZ MMM Got to see httptcoxeMnQv7YcRMMM  above 12760 can gain steam  Volume not impressing meTSN MAT QQQ XLE XLV XOP MS MMM ChartsinPlay Portfolio  changes in stops and sell advice  article later httptcoCW4YD3lpfBTSN MAT QQQ XLE XLV XOP MS MMM ChartsinPlay Portfolio  changes in stops and sell advice  article later httptco70ZRkrIBhJTSN MAT QQQ XLE XLV XOP MS MMM ChartsinPlay Portfolio  changes in stops and sell advice  article later httptcofyoFkbgRZcTSN MAT QQQ XLE XLV XOP MS MMM ChartsinPlay Portfolio  changes in stops and sell advice  article later httptcoWy7KgCw9YzMMM  3M  Analyst Is Right Limited Short To MediumTerm Appeal gt httptcoTuJoiXQMAf stock stocks MMMMMM 3M Co MMM 3M  Analyst Is Right Limited Short To MediumTerm Appeal httptcotT8fZdmp0O3M Co MMM 3 Reasons To Buy 3M E I Du Pont De Nemours And Co  MMM httptcox9HB2qh8Xs3M Co MMM 3M  Analyst Is Right Limited Short To MediumTerm Appeal MMM httptco0ZrdXLtIZB3M  Analyst Is Right Limited Short To MediumTerm Appeal httptcoeu5vDkFlui MMM3M  Analyst Is Right Limited Short To MediumTerm Appeal httptcoW8SwGHrC3E MMM\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive Bayesian Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can begin making a proper bag of words and performing sentiment analysis.  Many of the functions for this are contained in the file sentiment1.py, which we wrote and which we imported at the top.  We start with a make_xy function, where X is a matrix where each row represents a (stock,date) combination and each column represents a particular word, with the elements representing the frequency of that word in the bag of words.  The Y is just are performance indicator (1 for up for the day, 0 for down) which we made and attached to our dataframe earlier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, Y = sent.make_xy(data,vectorizer=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we use a Navie Bayes classifier as we did in the Bayesian Tomatoes problem set, split our data into a traininig set and a testing set, and check our accuracy on the training set and on the testing set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=22)\n",
      "clf = MultinomialNB()\n",
      "clf.fit(X_train,Y_train)\n",
      "clf.predict(X_test)\n",
      "print \"accuracy on testing set\",1-float(sum(abs(Y_test-clf.predict(X_test))))/float(len(Y_test))\n",
      "print \"accuracy on training set\",1-float(sum(abs(Y_train-clf.predict(X_train))))/float(len(Y_train))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy on testing set 0.516279069767\n",
        "accuracy on training set 0.973221117062\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So how did we do?  At first glance, 51.6% is not very good.  On the other hand, even a small edge in finance can be useful.  But do we have an edge?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"fraction that are positive on the day\",float(len(Y_test[Y_test==1]))/float(len(Y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fraction that are positive on the day 0.517829457364\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We actually did worse than just assuming every stock went up!  It turns out that we didn't gain any edge at all.  Particularly considering how high our accuracy was on the training set and how poor it is on the testing set, it looks like we have serious over-fitting issues, likely due to the smaller size of our data set, the sparsity of our word frequency, and a potentially weak underlying connnection from tweets to stock performance.  For example, people sometimes might tweet \"$GOOG is doing great today!\", but not all tweets are necessarily as clear.  Unlike movie reviews, the explicit purpose of a tweet is not to determine whether a stock is doing well or poorly, the way the movie reviews are explicit about whether a movie is good or bad.\n",
      "\n",
      "We'll try fitting for the best alpha and min_df parameters as we did in the problem set to see if this resolves these issues, although if the underlying problems are in the data we wouldn't expect this to work."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alphas = [0, .1, 1, 5, 10, 50, 100, 150, 200]\n",
      "min_dfs = [1e-6, 1e-7, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
      "\n",
      "#Find the best value for alpha and min_df, and the best classifier\n",
      "best_alpha = None\n",
      "best_min_df = None\n",
      "max_loglike = -np.inf\n",
      "\n",
      "#for alpha in alphas:\n",
      "for alpha in alphas:\n",
      "    print alpha#I print alpha as this takes a bit to run and I like to be able to see the progress\n",
      "    for min_df in min_dfs:         \n",
      "        vectorizer = CountVectorizer(min_df = min_df)       \n",
      "        X, Y = sent.make_xy(data, vectorizer)\n",
      "        clf = MultinomialNB(alpha=alpha)#should move outside of inner for loop\n",
      "        loglike=sent.cv_score(clf,X,Y,sent.log_likelihood)\n",
      "        #print loglike\n",
      "        if loglike>max_loglike:\n",
      "            max_loglike=loglike\n",
      "            print \"max_loglike\",max_loglike,\"alpha\",alpha,\"min_df\",min_df\n",
      "            best_alpha = alpha\n",
      "            best_min_df = min_df\n",
      "print \"best max_loglike\",max_loglike,\"best alpha\",best_alpha,\"best min_df\",best_min_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -701.863599147 alpha 0 min_df 0.1\n",
        "0.1\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -701.698320488 alpha 0.1 min_df 0.1\n",
        "1\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -700.218818186 alpha 1 min_df 0.1\n",
        "5\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -693.812946439 alpha 5 min_df 0.1\n",
        "10\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -686.175301985 alpha 10 min_df 0.1\n",
        "50\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -636.827815439 alpha 50 min_df 0.1\n",
        "100\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -595.120648905 alpha 100 min_df 0.1\n",
        "150\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -566.893075242 alpha 150 min_df 0.1\n",
        "200\n",
        "max_loglike"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -547.192426876 alpha 200 min_df 0.1\n",
        "best max_loglike -547.192426876 best alpha 200 best min_df 0.1\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the maximal parameters are absurd, particularly the very high best_min_df, so it looks like we can't salvage our model this way.  But is there anything else we can do?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Beyond Binary Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of movie reviews, Rotten Tomatoes compresses down \"thumbs up\", \"four out of five stars\", and other such positive reviews to just \"fresh\".  Similarly, we took all positive days and just converted them to \"up\", and all negative days and converting them to \"down\".  Seeing as how we're up against a shortage of data, it's sensible for us to avoid this strategy (which is what allowed us to do binary classification earlier) and instead keep more detailed performance information.\n",
      "\n",
      "We rebuild the performance indicator portion of our data set, instead looking at the fractional gain, clipping it from -1 to 1, then transforming it to the range 0 to 1.  So now a gain of 3% would be .56.  Again, we comment out the code to build the data set and just read it in from our previuos efforts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data = pd.read_csv('data.csv')\n",
      "#data = data[data['date']!=weekend_str[0]]\n",
      "#data = data[data['date']!=weekend_str[1]]\n",
      "#data,performance = perf2.check_performance_scaled(data)\n",
      "#data = data[data['performance']>=0]#only keep if performance is 0 to 1, not NaN\n",
      "##data.to_csv('data_perf_scaled.csv')\n",
      "data = pd.read_csv('data_perf_scaled.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As in our previous model, we now run make_xy.  Again, the X will be our bag of words results in matrix form, and our Y will be our performance indicators (which, recall, are continuous and based on the gain/loss.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, Y = sent.make_xy(data,vectorizer=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the binary classification model, we then ran this through a multinomial naive Bayes model to predict the probability of up/down for each stock. Since we aren't giving our model a binary classification anymore, this will no longer work.  However, we can still use a MultinomialNB model.  Now, instead of predicting the probability of 1/0, this will actually build a prediction of performance (such as, say, .52 for a 1% gain) based on the elements of our bag of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=22)\n",
      "clf = MultinomialNB()\n",
      "clf.fit(X_train,Y_train)\n",
      "#clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To check the accuracy of our model, we see which stocks it predicts a nontrivial gain for (using a tolerance of .2% movement) and then see if the direction predicted is correct."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predvec = clf.predict(X_test)\n",
      "indexpred = np.abs(predvec-.5)>0\n",
      "actual = np.divide(Y_test[indexpred]-.5,np.abs(Y_test[indexpred]-.5))*.5+.5\n",
      "predict = np.divide(clf.predict(X_test)[indexpred]-.5,np.abs(clf.predict(X_test)[indexpred]-.5))*.5+.5\n",
      "print \"accuracy on testing set\",1-float(sum(abs(actual-predict)))/float(len(actual))\n",
      "print \"portion of stocks with motion going up in testing set\",float(len(predict[predict>0]))/float(len(predict))\n",
      "predvec = clf.predict(X_train)\n",
      "indexpred = np.abs(predvec-.5)>0\n",
      "actual = np.divide(Y_train[indexpred]-.5,np.abs(Y_train[indexpred]-.5))*.5+.5\n",
      "predict = np.divide(clf.predict(X_train)[indexpred]-.5,np.abs(clf.predict(X_train)[indexpred]-.5))*.5+.5\n",
      "print \"accuracy on training set\",1-float(sum(abs(actual-predict)))/float(len(actual))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy on testing set 0.646341463415\n",
        "portion of stocks with motion going up in testing set 0.40243902439\n",
        "accuracy on training set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.981389578164\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And we've succeeded!  We still have a very high probability on our training set indicating over fitting issues, but it works out to give us accurate results on our testing set, enabling us to gain an edge.  The edge is not massive, as we are getting our predictions right about 65% of the time and the actual split between up stocks and downs tocks (among the stocks with sufficient movement) is 40/60, we have gained a several percent edge, which is considered very significant in the realm of finance.\n",
      "\n",
      "In our binary classification models, we could from here check the level of overfitting of our model by binning up predictions (10%-20% up, 20%-30% up, and so on) and seeing if our accuracy matches our predictions.  Unfortunately, one cost of using our Multinomial Naive Bayes model in this way is that we no longer have probability predictions, just direction predictions.  In essense, we've lost our error term, which is always something we'd like to avoid when possible in data analysis.  This also makes it difficult to tune our model, such as by carefully adjusting min_df or alpha parameters.  Still, if the benefit of losing tunability and error analysis is going from just noise to a useful prediction, we'll take it.\n",
      "\n",
      "One way we can check the reasonableness of our model, though, is by checking some words with strong negative and positive indications, as we did with Rotten Tomatoes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = CountVectorizer(min_df = 1e-3)\n",
      "#vec = CountVectorizer()\n",
      "text = [words for i,words in data.text.iteritems()]\n",
      "vec.fit(text)\n",
      "words = np.array(vec.get_feature_names())\n",
      "singles = np.eye(len(words))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_df, Y_df = sent.make_xy(data,vectorizer=vec)\n",
      "X_train_df,X_test_df,Y_train_df,Y_test_df = train_test_split(X_df,Y_df,test_size=0.33,random_state=22)\n",
      "clf_df = MultinomialNB()\n",
      "clf_df.fit(X_train_df,Y_train_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#clf_df = clf\n",
      "negativeindices = clf_df.predict_proba(sparse.csc_matrix(singles))[:,0].argsort()\n",
      "positiveindices = clf_df.predict_proba(sparse.csc_matrix(singles))[:,1].argsort()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "badwords=words[negativeindices]\n",
      "badprobs = clf_df.predict_proba(singles)[negativeindices,1]\n",
      "badprobs=badprobs[::-1]\n",
      "badwords=badwords[::-1]\n",
      "goodwords=words[positiveindices]\n",
      "goodwords=goodwords[::-1]\n",
      "goodprobs = clf_df.predict_proba(singles)[positiveindices,1]\n",
      "goodprobs = goodprobs[::-1]\n",
      "print \"negative words\",[(badwords[i],badprobs[i]) for i in range(10)]\n",
      "print \"positive words\",[(goodwords[i],goodprobs[i]) for i in range(10)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "negative words [(u'dva', 0.00065429865287221971), (u'davita', 0.00070301231619910383), (u'healthcare', 0.0006835894031860572), (u'partners', 0.0013194339424065626), (u'dialysis', 0.00072809514936371226), (u'dvadva', 0.00072592477320190742), (u'scan', 0.00072375005946349695), (u'rated', 0.00071782864597029678), (u'patients', 0.00071095170136868311), (u'health', 0.00066552469716559491)]\n",
        "positive words [(u'cpb', 0.032144008476641339), (u'soup', 0.013377734519007296), (u'options', 0.011244340670651403), (u'campbell', 0.010575715690765052), (u'day', 0.008966277117938088), (u'today', 0.0081946750241469728), (u'keeneonmarket', 0.007881447433107246), (u'over', 0.0078077819981858913), (u'iv', 0.00693413576281382), (u'up', 0.0068814076007903853)]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On both sides, it looks like there are a lot of words that were specific to just one or two stocks in our set.  We again have to recall that we're not predicting probabilities of going up or down but a magnitude in the direction, so one stock that really tanked on one day could easily have a large effect on our most extreme words.  This seems to be the case with so many pharmaceutical and health related words in our negative category.  On the positive side, in addition to a few that seem stock specific, we do seem some general terms we might have suspected such as \"options\" and \"up\".\n",
      "\n",
      "This is a long way from a predictive tool that could make money in real time, and it's not clear how good of a choice it is up against some of our other options we explored.  We did have the advantage of compiling all of the tweets for each day, including tweets that were about stock behavior that already happened, but this does in its current form provide a several percent edge and shows potential to be honed into a more realtime tool should one desire."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}